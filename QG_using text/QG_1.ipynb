{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAMcPvmijMr8"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/boudinfl/pke.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDE25m7rjSBL"
   },
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxOYCo_JjoZs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import contractions\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pke "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eIajy88jqds"
   },
   "outputs": [],
   "source": [
    "file_f = open('/content/AI.txt','r',errors='ignore')\n",
    "text=file_f.read()\n",
    "print(text)\n",
    "file_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjiVjj-tpMeJ"
   },
   "source": [
    "# Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHZkaScvoOPM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twDuoaE2jvOd"
   },
   "outputs": [],
   "source": [
    "string_1 = re.sub(r'[\\[]\\d\\d+[\\]]','',text)\n",
    "string_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYrpYmPVj4X5"
   },
   "outputs": [],
   "source": [
    "All_punct = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "All_punct=All_punct.replace(\".\",\"\")\n",
    "All_punct=All_punct.replace(\",\",\"\")\n",
    "for i in string_1:\n",
    "    if i in All_punct:\n",
    "        string_1 = string_1.replace(i, \"\")\n",
    "print(string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEL08g9ZpY05"
   },
   "outputs": [],
   "source": [
    "string_2 = re.sub(r'\\d+','',string_1)\n",
    "string_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1D3fBPQpmms"
   },
   "outputs": [],
   "source": [
    "# remove arabic words \n",
    "string_3 = re.sub(r\"[ุง-ู]\",'',string_2)\n",
    "string_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf3RnPi6ps_f"
   },
   "outputs": [],
   "source": [
    "#remove emails\n",
    "string_4 = re.sub(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\",'',string_3)\n",
    "string_4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHssdr3jpxhc"
   },
   "outputs": [],
   "source": [
    "# remove URL \n",
    "string_5 = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\",'',string_4)\n",
    "string_5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd-2YXv9p5jO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d5uMUL0j7WT"
   },
   "outputs": [],
   "source": [
    "string_5.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHpl3olNkB5a"
   },
   "outputs": [],
   "source": [
    "fixed_word = []\n",
    "for w in string_5.split() : \n",
    "    fixed_word.append(contractions.fix(w))\n",
    "    \n",
    "text_book = ' '.join(fixed_word)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp1rQcYNTIrw"
   },
   "outputs": [],
   "source": [
    "text_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key phrases extraction (using 5 algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STGmgwOGkFxa"
   },
   "outputs": [],
   "source": [
    "# 1 algorithm (topic rank) graph \n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be a simple \n",
    "# test string and preprocessing is carried out using spacy\n",
    "extractor.load_document(input=text_book, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4KjGwJAkMG4"
   },
   "outputs": [],
   "source": [
    "keyphrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVDNWP5ukPQ8"
   },
   "outputs": [],
   "source": [
    "# 2 algorithm (YAKE) statistical \n",
    "from pke.lang import stopwords\n",
    "\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "stoplist = stopwords.get('english')\n",
    "extractor.load_document(input=text_book,\n",
    "                        language='en',\n",
    "                        stoplist=stoplist,\n",
    "                        normalization=None)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8 \n",
    "keyphrases_1 = extractor.get_n_best(n=15, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdfZHWBRkSqW"
   },
   "outputs": [],
   "source": [
    "# 3 algortihm (Text Rank) graph \n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_book,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases_2 = extractor.get_n_best(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6R4LzVmkWmG"
   },
   "outputs": [],
   "source": [
    "# 4 algorithm (single rank) graph \n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a SingleRank extractor.\n",
    "extractor = pke.unsupervised.TfIdf()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_book,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk. In the graph, nodes are words of\n",
    "#    certain part-of-speech (nouns and adjectives) that are connected if\n",
    "#    they occur in a window of 10 words.\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases_3 = extractor.get_n_best(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VippoIbJkZO-"
   },
   "outputs": [],
   "source": [
    "# 5 algortihm (position rank) graph \n",
    "# define the valid Part-of-Speeches to occur in the graph\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# define the grammar for selecting the keyphrase candidates\n",
    "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "# 1. create a PositionRank extractor.\n",
    "extractor = pke.unsupervised.PositionRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_book,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the noun phrases up to 3 words as keyphrase candidates.\n",
    "extractor.candidate_selection(grammar=grammar,\n",
    "                              maximum_word_number=3)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk biaised with the position of the words\n",
    "#    in the document. In the graph, nodes are words (nouns and\n",
    "#    adjectives only) that are connected if they occur in a window of\n",
    "#    10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases_4 = extractor.get_n_best(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L7MUPSxkcZx"
   },
   "outputs": [],
   "source": [
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efpmERpGkivn"
   },
   "outputs": [],
   "source": [
    " # 6 algortihm (rake)\n",
    "from rake_nltk import Rake\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(text_book)\n",
    "rak =r.get_ranked_phrases_with_scores()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiBUhnV0lvgu"
   },
   "outputs": [],
   "source": [
    "rak "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5hi4es1l0AG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0nwzlyLrU4i"
   },
   "outputs": [],
   "source": [
    "#1 algorithm (topic rank) graph \n",
    "print(keyphrases)\n",
    "# 2 algorithm (YAKE) statistical \n",
    "print(keyphrases_1)\n",
    "# 3 algortihm (Text Rank) graph\n",
    "print(keyphrases_2)\n",
    "# 4 algorithm (single rank) graph \n",
    "print(keyphrases_3)\n",
    "# 5 algortihm (position rank) graph \n",
    "print(keyphrases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7gKsHw3sm74"
   },
   "outputs": [],
   "source": [
    "# 2 algorithm (YAKE) statistical \n",
    "print(keyphrases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTuHOzRtsqg8"
   },
   "outputs": [],
   "source": [
    "# 3 algortihm (Text Rank) graph\n",
    "print(keyphrases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLjIt3TCArQs"
   },
   "outputs": [],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most common keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfhYPcIsl0qe"
   },
   "outputs": [],
   "source": [
    "# most common key phrases \n",
    "common = {}\n",
    "\n",
    "for i,(key,value) in enumerate(keyphrases):\n",
    "     if common.get(key)!= None :\n",
    "          common[key] = common[key] +1 \n",
    "     else:\n",
    "          common[key] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQS_YiK_tGAP"
   },
   "outputs": [],
   "source": [
    "common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbCTNNeGl5Ko"
   },
   "outputs": [],
   "source": [
    "#common\n",
    "#s=[]\n",
    "#p=[]\n",
    "#for i ,j in common.items():\n",
    "#  if(type(i)!=tuple):\n",
    "#    s.append(i)\n",
    "#    p.append(j)\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2YztKKgLbSR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#q={}\n",
    "#for index,value in zip(s,p):\n",
    "#  q.update({index:value})\n",
    "#q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPViADjDMdgv"
   },
   "outputs": [],
   "source": [
    "for i,(key,value) in enumerate(keyphrases_1):\n",
    "     if common.get(key)!= None :\n",
    "          common[key] = common[key] +1 \n",
    "     else:\n",
    "          common[key] = 1\n",
    "\n",
    "#enumerate(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFklumg5uR6Q"
   },
   "outputs": [],
   "source": [
    "keyphrases_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nGrRMY6uG_g"
   },
   "outputs": [],
   "source": [
    "common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT1C7VncmEp_"
   },
   "outputs": [],
   "source": [
    "for i,(key,value) in enumerate(keyphrases_2):\n",
    "     if common.get(key)!= None :\n",
    "          common[key] = common[key] +1 \n",
    "     else:\n",
    "          common[key] = 1\n",
    "\n",
    "#enumerate(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuCiTI4ZB9gr"
   },
   "outputs": [],
   "source": [
    "common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lska3tUDmJD9"
   },
   "outputs": [],
   "source": [
    "for i,(key,value) in enumerate(keyphrases_3):\n",
    "     if common.get(key)!= None :\n",
    "          common[key] = common[key] +1 \n",
    "     else:\n",
    "          common[key] = 1\n",
    "\n",
    "#enumerate(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtC3lPZ7BOiZ"
   },
   "outputs": [],
   "source": [
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tVd9ky-mj5s"
   },
   "outputs": [],
   "source": [
    "for i,(key,value) in enumerate(keyphrases_4):\n",
    "     if common.get(key)!= None :\n",
    "          common[key] = common[key] +1 \n",
    "     else:\n",
    "          common[key] = 1\n",
    "\n",
    "#enumerate(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nhYlDCNmnX7"
   },
   "outputs": [],
   "source": [
    "common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIHR8OXXV7Ak"
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZP3pbjhVoBh"
   },
   "outputs": [],
   "source": [
    "common = sorted(common.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJe4yM4MV--u"
   },
   "outputs": [],
   "source": [
    "common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGQ7Srarmpie"
   },
   "outputs": [],
   "source": [
    "# common \n",
    "#t=[]\n",
    "#common.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5An2gvv1msAd"
   },
   "outputs": [],
   "source": [
    "#for (key,value) in rak:\n",
    " #    if key in rak:\n",
    "  #        common[key] += 1\n",
    "   #  else:\n",
    "    #      common[key] = 1\n",
    "#enumerate(rak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjpPzxbPmsj2"
   },
   "outputs": [],
   "source": [
    "common_list = []\n",
    "for i in common :\n",
    "  common_list.append(i[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-gbaVJwmzZk"
   },
   "outputs": [],
   "source": [
    "common_list=common_list[:24] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4utJjI_gZE0J"
   },
   "outputs": [],
   "source": [
    "common_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNrM6ptHvfm7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSTHcUMZKUqP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMqL6IwFm4HA"
   },
   "outputs": [],
   "source": [
    "#for i in common_list :\n",
    "#  if type(i) != str :\n",
    "#    common_list.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZaBIFV9nHBl"
   },
   "outputs": [],
   "source": [
    "#common_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PX96jgrnIkH"
   },
   "outputs": [],
   "source": [
    "# common_list.remove(178.90476190476193)\n",
    "# common_list.remove(66.16666666666666)\n",
    "# common_list.remove(36.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKrKw44Gnp5u"
   },
   "outputs": [],
   "source": [
    "#common_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBOelpddEBzq"
   },
   "outputs": [],
   "source": [
    "!pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhJzVgE7nrMH"
   },
   "outputs": [],
   "source": [
    "pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5507Oevnv0R"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61I8yrYUGJX6"
   },
   "outputs": [],
   "source": [
    "#common_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twOTIOkpHUvr"
   },
   "outputs": [],
   "source": [
    "#keyphrases[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnjLhRoyGrHW"
   },
   "outputs": [],
   "source": [
    "#keyphrasess = []\n",
    "#for i in keyphrases :\n",
    "#   keyphrasess = keyphrases[i][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A8NX5vnG_dM"
   },
   "outputs": [],
   "source": [
    "#keyphrasess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lksSFWGl1lXv"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!python -m spacy download en\n",
    "!pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
    "#!pip install spacy==2.1.3 --upgrade --force-reinstall\n",
    "!pip install -U nltk\n",
    "!pip install -U pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft8m1FHZ1GYd"
   },
   "outputs": [],
   "source": [
    "pip install summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1UbVfXyZqwz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnWG2tyS4mhD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UnMsY-E2xAF"
   },
   "outputs": [],
   "source": [
    "#from summarizer import summarize\n",
    "#summ_text =summarize('Machine learning', text_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UyhbV-S4nab"
   },
   "outputs": [],
   "source": [
    "#summ_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence mapping for each keyphrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaEO5ayony6G"
   },
   "outputs": [],
   "source": [
    "# t3ml df feha el key we el paragraph bta3o , el paragraph e5o4 3la el t5 transform\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "\n",
    "sentences = tokenize_sentences(text_book)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(common_list, sentences)\n",
    "        \n",
    "print (keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0Ka1QssyCPT"
   },
   "outputs": [],
   "source": [
    "keyword_sentence_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWtWSQ4CGTv8"
   },
   "outputs": [],
   "source": [
    "list(keyword_sentence_mapping.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ynwwNDPn0xX"
   },
   "outputs": [],
   "source": [
    "pip install python-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8BZ5RULEPaE"
   },
   "outputs": [],
   "source": [
    "!pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX_HJ6_5FHq3"
   },
   "outputs": [],
   "source": [
    "#text_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFsR1xlPEand"
   },
   "outputs": [],
   "source": [
    "#from keybert import KeyBERT\n",
    "#model =KeyBERT()\n",
    "#keyphrases=model.extract_keywords(text_book,top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_l1B7c4E1Aa"
   },
   "outputs": [],
   "source": [
    "#keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTtGro_XoyLu"
   },
   "outputs": [],
   "source": [
    "pip install pipelines==0.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9M6_FmTo1S4"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlJJX8r5o3ad"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmaUnIK_pDXK"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/patil-suraj/question_generation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo_1rCQqpGBr"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMXfBmyopJsM"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/amontgomerie/question_generator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFTHd7IXpWO3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0c8BpNgpMQB"
   },
   "outputs": [],
   "source": [
    "%cd question_generator/\n",
    "%load questiongenerator.py\n",
    "from questiongenerator import QuestionGenerator\n",
    "from questiongenerator import print_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19h4ivl7pPbd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#assert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7u-W5GUrSjc"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!python -m spacy download en\n",
    "!pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
    "!pip install spacy==2.1.3 --upgrade --force-reinstall\n",
    "!pip install -U nltk\n",
    "!pip install -U pywsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8keoy8rsXck"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QsUcF3XsicR"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDvQQXVDq1ne"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# import re\n",
    "# import random\n",
    "# from pywsd.similarity import max_similarity\n",
    "# from pywsd.lesk import adapted_lesk\n",
    "# from pywsd.lesk import simple_lesk\n",
    "# from pywsd.lesk import cosine_lesk\n",
    "# from nltk.corpus import wordnet as wn\n",
    "\n",
    "# # Distractors from Wordnet\n",
    "# def get_distractors_wordnet(syn,word):\n",
    "#     distractors=[]\n",
    "#     word= word.lower()\n",
    "#     orig_word = word\n",
    "#     if len(word.split())>0:\n",
    "#         word = word.replace(\" \",\"_\")\n",
    "#     hypernym = syn.hypernyms()\n",
    "#     if len(hypernym) == 0: \n",
    "#         return distractors\n",
    "#     for item in hypernym[0].hyponyms():\n",
    "#         name = item.lemmas()[0].name()\n",
    "#         #print (\"name \",name, \" word\",orig_word)\n",
    "#         if name == orig_word:\n",
    "#             continue\n",
    "#         name = name.replace(\"_\",\" \")\n",
    "#         name = \" \".join(w.capitalize() for w in name.split())\n",
    "#         if name is not None and name not in distractors:\n",
    "#             distractors.append(name)\n",
    "#     return distractors\n",
    "\n",
    "# def get_wordsense(sent,word):\n",
    "#     word= word.lower()\n",
    "    \n",
    "#     if len(word.split())>0:\n",
    "#         word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "#     synsets = wn.synsets(word,'n')\n",
    "#     if synsets:\n",
    "#         wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "#         adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "#         lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "#         return synsets[lowest_index]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Distractors from http://conceptnet.io/\n",
    "# def get_distractors_conceptnet(word):\n",
    "#     word = word.lower()\n",
    "#     original_word= word\n",
    "#     if (len(word.split())>0):\n",
    "#         word = word.replace(\" \",\"_\")\n",
    "#     distractor_list = [] \n",
    "#     url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "#     obj = requests.get(url).json()\n",
    "\n",
    "#     for edge in obj['edges']:\n",
    "#         link = edge['end']['term'] \n",
    "\n",
    "#         url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "#         obj2 = requests.get(url2).json()\n",
    "#         for edge in obj2['edges']:\n",
    "#             word2 = edge['start']['label']\n",
    "#             if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "#                 distractor_list.append(word2)\n",
    "                   \n",
    "#     return distractor_list\n",
    "\n",
    "# key_distractor_list = {}\n",
    "\n",
    "# for keyword in keyword_sentence_mapping:\n",
    "#     wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "#     if wordsense:\n",
    "#         distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "#         if len(distractors) ==0:\n",
    "#             distractors = get_distractors_conceptnet(keyword)\n",
    "#         if len(distractors) != 0:\n",
    "#             key_distractor_list[keyword] = distractors\n",
    "#     else:\n",
    "        \n",
    "#         distractors = get_distractors_conceptnet(keyword)\n",
    "#         if len(distractors) != 0:\n",
    "#             key_distractor_list[keyword] = distractors\n",
    "\n",
    "# index = 1\n",
    "# print (\"#############################################################################\")\n",
    "# print (\"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
    "# print (\"#############################################################################\\n\\n\")\n",
    "# for each in key_distractor_list:\n",
    "#     sentence = keyword_sentence_mapping[each][0]\n",
    "#     pattern = re.compile(each, re.IGNORECASE)\n",
    "#     output = pattern.sub( \" _______ \", sentence)\n",
    "#     print (\"%s)\"%(index),output)\n",
    "#     choices = [each.capitalize()] + key_distractor_list[each]\n",
    "#     top4choices = choices[:4]\n",
    "#     random.shuffle(top4choices)\n",
    "#     optionchoices = ['a','b','c','d']\n",
    "#     for idx,choice in enumerate(top4choices):\n",
    "#         print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
    "#     print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "#     index = index + 1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO21fYVEdyoz"
   },
   "outputs": [],
   "source": [
    "keyword_sentence_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_eCo0gTeq2C"
   },
   "outputs": [],
   "source": [
    "phrases = []\n",
    "paragraph = []\n",
    "for i in keyword_sentence_mapping : \n",
    "  phrases.append(i)\n",
    "  paragraph.append(keyword_sentence_mapping[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VEdxjU9fDzB"
   },
   "outputs": [],
   "source": [
    "phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KggecpQLfHd2"
   },
   "outputs": [],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAOz-IhJfOum"
   },
   "outputs": [],
   "source": [
    "key_dics = {\n",
    "    'Phrases':phrases,\n",
    "    'Paragraphs':paragraph\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpRY_7J8fVYi"
   },
   "outputs": [],
   "source": [
    "df= pd.DataFrame(key_dics)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHpzhqz6fd09"
   },
   "outputs": [],
   "source": [
    "df.to_csv('sentence_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question generator by using text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMfJLAT8pflY"
   },
   "outputs": [],
   "source": [
    "qg = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAysiVMVpiv2"
   },
   "outputs": [],
   "source": [
    "qa_list = qg.generate(\n",
    "    text_book, \n",
    "    num_questions=20, \n",
    "    answer_style='all'\n",
    ")\n",
    "print_qa(qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34ql7O9Mp4Jy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IWurAq7gwzs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETsKlFiwwrpG"
   },
   "outputs": [],
   "source": [
    "qa_list = qg.generate(\n",
    "    text, \n",
    "    num_questions=20, \n",
    "    answer_style='all'\n",
    ")\n",
    "print_qa(qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLU72jABxdKd"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGYPxK-11MLm"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwqOSsut1V6q"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/patil-suraj/question_generation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47kbCMRt1ZN1"
   },
   "outputs": [],
   "source": [
    "%cd question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqJmE1wj1dhK"
   },
   "outputs": [],
   "source": [
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqlU9g_Y1gd6"
   },
   "outputs": [],
   "source": [
    "# single_task \n",
    "nlp = pipeline(\"question-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKOkdYOQ1nVM"
   },
   "outputs": [],
   "source": [
    "#nlp(text_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHD7GqWt1yrT"
   },
   "outputs": [],
   "source": [
    "nlp_1 = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znQtzH2R1_Q1"
   },
   "outputs": [],
   "source": [
    "# piplines should take substring\n",
    "#nlp_1(text_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAunroRX2TYJ"
   },
   "outputs": [],
   "source": [
    "keyword_sentence_mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM3mfDoE9wtZ"
   },
   "outputs": [],
   "source": [
    "for i in keyword_sentence_mapping.values():\n",
    "  print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFD7ziwQ7ne9"
   },
   "outputs": [],
   "source": [
    "v = list(keyword_sentence_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYhcTT3L3OtQ"
   },
   "outputs": [],
   "source": [
    "#for i in v :\n",
    " # print(nlp_1(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74gHBXy23EeW"
   },
   "outputs": [],
   "source": [
    "#qa_list = qg.generate(\n",
    " #   text, \n",
    "  #  num_questions=20, \n",
    "   # answer_style='all'\n",
    "#)\n",
    "#print_qa(qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYL13ystz64D"
   },
   "outputs": [],
   "source": [
    "!pip install summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4pOekuAz8zt"
   },
   "outputs": [],
   "source": [
    "# used many question generators by ur dataframe at least 2 \n",
    "# used 2 queastion answering by ur dataframe\n",
    "# used many anserwing generators by ur dataframe at least 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWggCn1o0Kqd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLTX7a0Gn0dE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY3ZRmIjq-lo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMTUQPaDx0+v4oIZ+s3GQyq",
   "collapsed_sections": [],
   "history_visible": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
